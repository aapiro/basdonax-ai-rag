version: '3.6'

services:
  ollama:
    image: ollama/ollama:latest
    volumes:
      - /ollama/models:/home/models
    environment:
      - OLLAMA_MODELS=/home/models
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [gpu]

  chroma:
    image: chromadb/chroma:0.5.1.dev111
    volumes:
      - index_data:/chroma/.chroma/index
    ports:
      - 8000:8000
    networks:
      - net

  nvidia:
    image: nvidia/cuda:12.3.1-base-ubuntu20.04
    command: nvidia-smi
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  ui:
    build: ./app
    ports:
      - 8080:8080
    volumes:
      - ./app:/app
    depends_on:
      - nvidia
      - ollama
      - chroma
    environment:
      - MODEL=deepseek-r1:70b
      - EMBEDDINGS_MODEL_NAME=all-MiniLM-L6-v2
      - TARGET_SOURCE_CHUNKS=5
    extra_hosts:
      - "host.docker.internal:host-gateway"

volumes:
  index_data:

networks:
  net:
